{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3a\n",
    "\n",
    "* Please submit your assignment (notebooks of parts 3a and 3b + Python modules) as **a single .zip file** using Canvas (Assignments --> Assignment 3). Please put the notebooks for Assignment 3a and 3b as well as the Python modules (files ending with .py) in one folder, which you call ASSIGNMENT_3_FIRSTNAME_LASTNAME. Please zip this folder and upload it as your submission.\n",
    "\n",
    "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
    "\n",
    "**IMPORTANT NOTE**:\n",
    "* The students who follow the Bachelor version of this course, i.e., the course Introduction to Python for Humanities and Social Sciences (L_AABAALG075) as part of the minor Digital Humanities, do **NOT have to do Exercises 3 and 4 of Assignment 3b**\n",
    "* The other students, who follow the Master version of Programming in Python for Text Analysis (L_AAMPLIN021), are required to **DO Exercises 3 and 4 of Assignment 3b**\n",
    "\n",
    "If you have **questions** about this topic, please contact us **(cltl.python.course@gmail.com)**. Questions and answers will be collected on Piazza, so please check if your question has already been answered first.\n",
    "\n",
    "\n",
    "In this block, we covered a lot of ground:\n",
    "\n",
    "* Chapter 12 - Importing external modules \n",
    "* Chapter 13 - Working with Python scripts\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text \n",
    "\n",
    "\n",
    "In this assignment, you will first complete a number of small exercises about each chapter to make sure you are familiar with the most important concepts. In the second part of the assignment, you will apply your newly acquired skills to write your very own text processing program (ASSIGNMENT-3b) :-). But don't worry, there will be instructions and hints along the way. \n",
    "\n",
    "\n",
    "**Can I use external modules other than the ones treated so far?**\n",
    "\n",
    "For now, please try to avoid it. All the exercises can be solved with what we have covered in block I, II, and III. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1:\n",
    "\n",
    "Define a function called `split_sort_text` which takes one positional parameter called **text** (a string).\n",
    "\n",
    "The function:\n",
    "* splits the string on a space character, i.e., ' '  [not all whitespace!]\n",
    "* returns all the unique words in alphabetical order as a list.\n",
    "\n",
    "* Hint 1: There is a specific python container which does not allow for duplicates and simply removes them. Use this one. \n",
    "* Hint 2: There is a built-in function which sorts items in an iterable called 'sorted'. Look at the documentation to see how it is used. \n",
    "* Hint 3: Don't forget to write a docstring. Please make sure that the docstring generally explains with the input is, what the function does, and what the function returns. If you want, but this is not needed to receive full points, you can use [reStructuredText](http://docutils.sourceforge.net/rst.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'This', 'a', 'away', 'far', 'golden', 'home', 'kingdom,', 'many', 'river', 'river.', 'swans.', 'there', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "# Defining the function split_sort_text with one positional parameter 'text' with the data type str \n",
    "def split_sort_text(text: str, /):\n",
    "    \"\"\"\n",
    "    Takes takes a string of text, converts it to a list of unique words used in the text \n",
    "    and returns this list sorted alphabetically.\n",
    "    \n",
    "    :param text: a string of text \n",
    "    :return: a list of words sorted alphabetically\n",
    "    \"\"\"\n",
    "    # using the split method to split the string into a list of words\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # converting the list to a set to obtain the unique words\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    # converting the set back to get a list of unique words\n",
    "    list_unique_words = list(unique_words) \n",
    "    \n",
    "    # sort the list on alphabetical order using the built-in sorted function\n",
    "    sorted_list_unique_words = sorted(list_unique_words) \n",
    "    \n",
    "    return sorted_list_unique_words\n",
    "\n",
    "# We use a sample text from Assignment 2 to test our function with\n",
    "a_text = \"In a far away kingdom, there was a river. This river was home to many golden swans.\"\n",
    "\n",
    "# We call the function and assign the returned list to sorted_unique_words_a_text\n",
    "sorted_unique_words_a_text = split_sort_text(a_text)\n",
    "\n",
    "# We print the list\n",
    "print(sorted_unique_words_a_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "NLTK offers a way of using WordNet in Python. Do some research (using google, because quite frankly, that's what we do very often) and see if you can find out how to import it. WordNet is a computational lexicon which organizes words according to their senses (collected in synsets). See if you can print all the **synset definitions** of the lemma **dog**.\n",
    "\n",
    "Make sure you have run the following cell to make sure you have installed WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/koenvanderpool/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this exercise I used the following source: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# uncomment the following line to download material including WordNet\n",
    "nltk.download('book')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the synset definitions of the lemma dog\n",
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  3\n",
    "\n",
    "#### a.) Define a function called `my_word_count`, which determines how often each word occurs in a string and returns the result as a python dictionary. Do not use NLTK just yet. Find a way to test it. \n",
    "\n",
    "* Write a helper-function called `preprocess`, which removes the punctuation specified by the user, and returns the same string without the unwanted characters. You should call the function `preprocess` inside the `my_word_count` function.\n",
    "\n",
    "* Remember that there are string methods that you can use to get rid of unwanted characters. Test the `preprocess` function using the following string `'this is a (tricky) test'` by attempting to remove the opening and closing parentheses.\n",
    "\n",
    "* Remember how we used dictionaries to count words? If not, have a look at Chapter 10 - Dictionaries. \n",
    "\n",
    "* Make sure you split the string on a space character ' '. You then can loop over the list to count the words.\n",
    "\n",
    "* Test your function using an example string, which will tell you whether it fulfills the requirements (remove punctuation, split, count).\n",
    "\n",
    "#### b.) Create a python script \n",
    "\n",
    "Use your editor to create a Python script called **count_words.py**. Place the function definition of the **my_word_count** function in **count_words.py**. Also call the function **my_word_count** in this file to test it. Print the results (you can choose the parameters of this call). Place your helper function definition, i.e., **preprocess**, in a separate script called **utils_3a.py**. Import your helper function **preprocess** into count_words.py. Test whether everything works as expected by calling the script count_words.py from the terminal.\n",
    "\n",
    "The function **preprocess** preprocesses the text by removing characters that are unwanted by the user. **preprocess** is called within the **my_word_count**. The function **my_word_count** uses the output from the preprocess function and creates a dictionary in which the key is a word and the value is the frequency of the word.\n",
    "\n",
    "**Please submit these scripts together with the notebooks**.\n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a tricky test\n"
     ]
    }
   ],
   "source": [
    "# Feel free to use this cell to try out your code. \n",
    "\n",
    "# a) Testing the function preprocess with the text string in the description\n",
    "\n",
    "###Taken from Feedback Session Block II - 24 september 2023\n",
    "\n",
    "# I took the code named clean_text_general from the feedback session and I made \n",
    "# very small changes in the naming of the variables. The code in the feedback \n",
    "# session looked already quite similar in idea to the code I wrote for Assignment 2, \n",
    "# but the one in the feedback session is a bit more efficient and clear, so \n",
    "# therefore I use that one.\n",
    "\n",
    "def preprocess(text: str, punct_to_remove, /):\n",
    "    \"\"\"\n",
    "    Removes punctuation characters from a given text string.\n",
    "    \n",
    "    :param text: a string of text you wish to preprocess\n",
    "    :param punct_to_remove: the punctuation characters you wish to remove\n",
    "    :return: a preprocessed string of text removed from its punctuation characters\n",
    "    \"\"\"\n",
    "    \n",
    "    # assigning the text to a new variable to improve readability and clarity of the code\n",
    "    preprocessed_text = text\n",
    "    \n",
    "    # iteration over all punctuation characters\n",
    "    for punct in punct_to_remove:\n",
    "        preprocessed_text = preprocessed_text.replace(punct, '') # replace all punctuation characters in text with empty str\n",
    "\n",
    "    # return the preprocessed text\n",
    "    return preprocessed_text\n",
    "\n",
    "###\n",
    "\n",
    "# We assign the string to the variable tricky_test\n",
    "tricky_test = 'this is a (tricky) test'\n",
    "\n",
    "# We call the function preprocess to remove parentheses and assign the resulting string to preprocessed_tricky_test\n",
    "preprocessed_tricky_test = preprocess(tricky_test, {'(', ')'})\n",
    "\n",
    "# We print the result\n",
    "print(preprocessed_tricky_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Look': 1, 'at': 1, 'that': 1, 'dog': 2, 'he': 1, 'exclaimed': 1, 'I': 1, 'have': 1, 'never': 1, 'seen': 1, 'such': 1, 'a': 1, 'Have': 1, 'you': 1}\n"
     ]
    }
   ],
   "source": [
    "# a) Testing the function my_word_count with an example string\n",
    "\n",
    "# Defining the function with two positional parameters\n",
    "def my_word_count(text: str, punct_to_remove, /):\n",
    "    \"\"\"\n",
    "    Given a string of text, this function cleans the text from its punctuation and returns \n",
    "    a dictionary containing the words used it the text and their respective counts. \n",
    "    \n",
    "    :param text: a string of text for which you want to determine the word count\n",
    "    :param punct_to_remove: the punctuation characters you wish to remove from the text\n",
    "    :return: a dictionary of the words used in the text and their respective counts\n",
    "    \"\"\"\n",
    "    # initialise dictionary\n",
    "    word2freq = {}\n",
    "    \n",
    "    # call the function preprocess to have the string text cleaned from punctuation\n",
    "    preprocessed_text = preprocess(text, punct_to_remove) \n",
    "    \n",
    "    # split the preprocessed text into words\n",
    "    words = preprocessed_text.split(' ')\n",
    "    \n",
    "    # loop over the words to count them and add them to the dictionary\n",
    "    for word in words:\n",
    "        if word in word2freq:\n",
    "            word2freq[word] += 1\n",
    "        else:\n",
    "            word2freq[word] = 1\n",
    "            \n",
    "    # returning the dictionary        \n",
    "    return word2freq\n",
    "\n",
    "# We assign a string sentence to the variable a_sentence\n",
    "a_sentence = '\"Look at that dog!\" he exclaimed. \"I have never seen such a dog. Have you?\"'\n",
    "\n",
    "# We call the function and assign the resulting string to word_count_a_sentence\n",
    "word_count_a_sentence = my_word_count(a_sentence, {',', '.', '\"', '?', '!', ':', ';'})\n",
    "\n",
    "# We print the result\n",
    "print(word_count_a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) # You can test the function my_word_count with another piece of text by calling the script count_words.py \n",
    "# from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "**Playing with lyrics**\n",
    "\n",
    "a.) Write a function called `load_text`, which opens and reads a file and returns the text in the file. It should have the file path as a parameter. Test it by loading this file: ../Data/lyrics/walrus.txt\n",
    "\n",
    "* Hint: remember it is best practice to use a context manager\n",
    "* Hint: **FileNotFoundError**: This means that the path you provide does not lead to an existing file on your computer. Please carefully study Chapter 14. Please determine where the notebook or Python module that you are working with is located on your computer. Try to determine where Python is looking if you provide a path such as “../Data/lyrics/walrus.txt”. Try to go from your notebook to the location on your computer where Python is trying to find the file. One tip: if you did not store the Assignments notebooks 3a and 3b in the folder “Assignments”, you would get this error.\n",
    "\n",
    "b.) Write a function called `replace_walrus`, which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' (make sure to account for upper and lower case - it is fine to transform everything to lower case). The function should write the new version of the song to a file called 'walrus_hippo.txt and stored in ../Data/lyrics. \n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I Am The Walrus\"\n",
      "(\"Magical Mystery Tour\" Version)\n",
      "\n",
      "I am he\n",
      "As you are he\n",
      "As you are me\n",
      "And we are all together\n",
      "\n",
      "See how they run\n",
      "Like pigs from a gun\n",
      "See how they fly\n",
      "I'm crying\n",
      "\n",
      "Sitting on a cornflake\n",
      "Waiting for the van to come\n",
      "Corporation tee shirt\n",
      "Stupid bloody Tuesday\n",
      "Man, you been a naughty boy\n",
      "You let your face grow long\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen, (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Mister city p'liceman sitting pretty\n",
      "Little p'licemen in a row\n",
      "See how they fly\n",
      "Like Lucy in the sky\n",
      "See how they run\n",
      "I'm crying\n",
      "I'm crying, I'm crying, I'm crying\n",
      "\n",
      "Yellow matter custard\n",
      "Dripping from a dead dog's eye\n",
      "Crabalocker fishwife pornographic priestess\n",
      "Boy you been a naughty girl\n",
      "You let your knickers down\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Sitting in an English\n",
      "Garden waiting for the sun\n",
      "If the sun don't come\n",
      "You get a tan from standing in the English rain\n",
      "\n",
      "I am the eggman\n",
      "They are the eggmen\n",
      "I am the walrus\n",
      "Goo goo g' joob g' goo goo g' joob\n",
      "\n",
      "Expert texpert choking smokers\n",
      "Don't you think the joker laughs at you?\n",
      "See how they smile\n",
      "Like pigs in a sty, see how they snied\n",
      "I'm crying\n",
      "\n",
      "Semolina pilchards\n",
      "Climbing up the Eiffel Tower\n",
      "Element'ry penguin singing Hare Krishna\n",
      "Man, you should have seen them kicking Edgar Allan Poe\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "Goo goo g' joob\n",
      "G' goo goo g' joob\n",
      "Goo goo g' joob, goo goo g' goo g' goo goo g' joob joob\n",
      "Joob joob...\n"
     ]
    }
   ],
   "source": [
    "# a) Defining a function with one positional parameter\n",
    "\n",
    "def load_text(file_path: str, /):\n",
    "    \"\"\"\n",
    "    Opens a file at the specified path, reads its contents, and returns the text as a string.\n",
    "    \n",
    "    :param file_path: the pathname of the file you want to open\n",
    "    :return: the content of the file as a string of text\n",
    "    \"\"\"\n",
    "    # using a context manager to access the content of the file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text  = file.read() # read the entire file and assign it to the variable 'text'\n",
    "    \n",
    "    # return the string of text\n",
    "    return text\n",
    "\n",
    "# load the song lyrics from its file and assign the returned string to the variable 'content'\n",
    "content = load_text(\"../Data/lyrics/walrus.txt\")\n",
    "\n",
    "# print 'content' to test the function\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Defining a function with one positional parameter and one keyword parameter\n",
    "\n",
    "def replace_walrus(lyrics: str, /, replacement_word=\"hippo\"):\n",
    "    \"\"\"\n",
    "    Replaces the word \"walrus\" in the song lyrics with the (default) word \"hippo\".\n",
    "    \n",
    "    :param lyrics: string of song lyrics\n",
    "    :return: a string of lyrics in which the word \"walrus\" is replaced with the (default) word \"hippo\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercase all the song lyrics\n",
    "    modified_lyrics = lyrics.lower()\n",
    "    \n",
    "    # replace the word \"walrus\" with the replacement_word and assign the modified lyrics to a variable\n",
    "    modified_lyrics = modified_lyrics.replace(\"walrus\", replacement_word)\n",
    "    \n",
    "    # using a context manager to write the modified lyrics to a file\n",
    "    with open(\"../Data/lyrics/walrus_hippo.txt\", \"w\") as outfile:\n",
    "        outfile.write(modified_lyrics)\n",
    "\n",
    "# calling the function with the song lyrics to replace the word \"hippo\" and store in a file\n",
    "replace_walrus(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing text with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Building a simple NLP pipeline**\n",
    "\n",
    "For this exercise, you will need NLTK. Don't forget to import it. \n",
    "\n",
    "Write a function called `tag_text`, which takes raw text as input and returns the tagged text. To do this, make sure you follow the steps below:\n",
    "\n",
    "* Tokenize the text. \n",
    "\n",
    "* Perform part-of-speech tagging on the list of tokens. \n",
    "\n",
    "* Return the tagged text\n",
    "\n",
    "\n",
    "Then test your function using the text snipped below (`test_text`) as input.\n",
    "\n",
    "Please note that the tags may not be correct and that this is not a mistake on your end, but simply NLP tools not being perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Shall I compare thee to a summer's day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer's lease hath all too short a date:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shall', 'NN'), ('I', 'PRP'), ('compare', 'VBP'), ('thee', 'JJ'), ('to', 'TO'), ('a', 'DT'), ('summer', 'NN'), (\"'s\", 'POS'), ('day', 'NN'), ('?', '.'), ('Thou', 'NNP'), ('art', 'RB'), ('more', 'RBR'), ('lovely', 'RB'), ('and', 'CC'), ('more', 'JJR'), ('temperate', 'NN'), (':', ':'), ('Rough', 'NNP'), ('winds', 'NNS'), ('do', 'VBP'), ('shake', 'VB'), ('the', 'DT'), ('darling', 'VBG'), ('buds', 'NNS'), ('of', 'IN'), ('May', 'NNP'), (',', ','), ('And', 'CC'), ('summer', 'NN'), (\"'s\", 'POS'), ('lease', 'NN'), ('hath', 'NN'), ('all', 'DT'), ('too', 'RB'), ('short', 'JJ'), ('a', 'DT'), ('date', 'NN'), (':', ':')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# defining a function with one positional parameter\n",
    "def tag_text(text: str, /):\n",
    "    \"\"\"\n",
    "    Performs tagging on a given string of text and returns the tagged text.\n",
    "    \n",
    "    :param text: a string of text\n",
    "    :return: the tagged text\n",
    "    \"\"\"\n",
    "    # tokenizing the given text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # tagging the tokens\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # returned the tagged text\n",
    "    return tagged_tokens\n",
    "\n",
    "# calling the function and assign the tagged text to the variable tag_test_text\n",
    "tag_test_text = tag_text(test_text)\n",
    "\n",
    "# print the tagged text\n",
    "print(tag_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "6.a) Explain in your own words the difference between the global and the local scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scope in this regard refers to accessibility. In order to explain the difference between global and local scope, we take the creation of a variable as an example. If a variable is created globally, this means that it can be accessed throughout the program even in local evironments, such as a function. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# a global variable is made\n",
    "number = 5\n",
    "\n",
    "def example_function():\n",
    "    # we access the global variable from a local environment and print it\n",
    "    print(number)\n",
    "\n",
    "# calling the function results in the number to be printed\n",
    "example_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a global variable can be accessed in the whole program, even in local environment. However, creations in a local environments remain only accessible in the local scope. For instance, when a variable is created locally, we can only use it in the local environment, so not in the whole programe. Here is an example to demonstate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'another_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     another_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(another_number)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manother_number\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'another_number' is not defined"
     ]
    }
   ],
   "source": [
    "def another_example_function():\n",
    "    another_number = 5\n",
    "    print(another_number)\n",
    "\n",
    "print(another_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to print the variable another_number from a global environment, we get an error that the variable is not defined. This is because we are trying to access a local variable from a global environment, but in the global environment this variable does not \"exist\". The variable only exists in the local scope. Having distinction between global and local scope is very handy for larger projects. Often you want to write different functions that do different operations to similar pieces of data. Being able to do operations in a local environment, keeps your code clean and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.b) What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening a file in 'w' mode is similar to opening a file in 'a' mode when the file does not yet exist, namely, the file will be created and you can write data to it. However, when the file does exist, the two modes differ. Opening an already existing  file in 'w' mode will empty the file of its current content and, if you want to write data to it, overwrite it. On the other hand, if you open in 'a' mode, you can add (or append) data to the already existing file. When you open an existing file in 'a' mode, a pointer is placed at the end of the file and when you write to it, the data is added at this pointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
